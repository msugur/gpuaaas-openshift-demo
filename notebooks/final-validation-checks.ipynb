{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "acca9bbc",
   "metadata": {},
   "source": [
    "# ✅ Final Validation Notebook: GPUaaS on OpenShift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030f587a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cceefa77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify access to inference endpoint\n",
    "import requests, json\n",
    "\n",
    "url = \"http://<inference-service-url>/v2/models/onnx-model/infer\"  # Replace with actual route\n",
    "payload = json.load(open(\"../payloads/sample-inference-payload.json\"))\n",
    "\n",
    "try:\n",
    "    response = requests.post(url, json=payload)\n",
    "    print(\"Status Code:\", response.status_code)\n",
    "    print(\"Response:\", response.json())\n",
    "except Exception as e:\n",
    "    print(\"Error:\", str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13eb5c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPU is being utilized (basic)\n",
    "!nvidia-smi --query-compute-apps=pid,process_name,used_memory --format=csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae466efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL: Query Prometheus for GPU usage (if Prometheus is accessible)\n",
    "# from prometheus_api_client import PrometheusConnect\n",
    "# prom = PrometheusConnect(url=\"http://<prometheus-url>\", disable_ssl=True)\n",
    "# query = 'DCGM_FI_DEV_GPU_UTIL'\n",
    "# prom.custom_query(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141eae83",
   "metadata": {},
   "source": [
    "✅ **Conclusion:** If inference succeeds and GPU shows usage, your GPUaaS pipeline is working end-to-end."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
