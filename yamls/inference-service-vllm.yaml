apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  name: onnx-vllm-inference
  namespace: gpu-dsp
spec:
  predictor:
    serviceAccountName: default
    containers:
      - name: kserve-container
        image: quay.io/opendatahub/odh-inference-service:vllm-latest
        args:
          - "--model"
          - "s3://onnx-models/model.onnx"
          - "--model-format"
          - "onnx"
        env:
          - name: AWS_ACCESS_KEY_ID
            valueFrom:
              secretKeyRef:
                name: s3storageconfig
                key: AWS_ACCESS_KEY_ID
          - name: AWS_SECRET_ACCESS_KEY
            valueFrom:
              secretKeyRef:
                name: s3storageconfig
                key: AWS_SECRET_ACCESS_KEY
          - name: S3_ENDPOINT
            value: "https://minio-api-minio.apps.cluster-fcsfq.fcsfq.sandbox1242.opentlc.com"
        ports:
          - containerPort: 8080