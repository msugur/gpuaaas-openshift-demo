{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2fb1db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Deploy KServe InferenceService using oc\n",
    "import subprocess\n",
    "yaml = '''\n",
    "apiVersion: serving.kserve.io/v1beta1\n",
    "kind: InferenceService\n",
    "metadata:\n",
    "  name: onnx-model-vllm\n",
    "  namespace: gpu-dsp\n",
    "spec:\n",
    "  predictor:\n",
    "    model:\n",
    "      modelFormat:\n",
    "        name: onnx\n",
    "      runtime: kserve-onnxruntime\n",
    "      storage:\n",
    "        key: s3storageconfig\n",
    "        path: onnx-models/your-model.onnx\n",
    "'''\n",
    "with open('inference-service-vllm.yaml', 'w') as f:\n",
    "    f.write(yaml)\n",
    "subprocess.run(['oc', 'apply', '-f', 'inference-service-vllm.yaml', '-n', 'gpu-dsp'])\n",
    "print('âœ… InferenceService deployed.')"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
